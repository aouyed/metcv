{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will attempt my filter for the first time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    def __init__(self,  scale_factor):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "    def forward(self, x):\n",
    "        return F.interpolate(x, scale_factor=self.scale_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.conv1=nn.Conv2d(2, 16, 3, stride=2, padding=1)\n",
    "        self.conv2=nn.Conv2d(16, 32, 3, stride=2, padding=1)\n",
    "        self.conv3=nn.Conv2d(32, 64, 3)\n",
    "            \n",
    "        self.deconv1=nn.ConvTranspose2d(64, 32, 3)\n",
    "        self.deconv2=nn.ConvTranspose2d(32, 16, 3, stride=(2,2), padding=1, output_padding=1)\n",
    "        self.deconv3=nn.ConvTranspose2d(16, 2, 3, stride=(2,2), padding=1, output_padding=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),(2,2))\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)),(2,2))\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        \n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        x = torch.sigmoid(self.deconv3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "concatenating dataframes for all dates for further analysis:\n",
      "2006-07-01 06:00:00\n",
      "2006-07-01 06:00:00\n",
      "2006-07-01 07:00:00\n",
      "2006-07-01 07:00:00\n",
      "speed_error\n",
      "2.9248843\n"
     ]
    }
   ],
   "source": [
    "from viz import amv_analysis as aa\n",
    "from viz import dataframe_calculators as dfc \n",
    "import datetime\n",
    "import pickle \n",
    "import metpy\n",
    "import numpy as np\n",
    "import torch\n",
    "dict_path = '../data/interim/dictionaries/dataframes.pkl'\n",
    "dataframes_dict = pickle.load(open(dict_path, 'rb'))\n",
    "\n",
    "\n",
    "start_date=datetime.datetime(2006,7,1,6,0,0,0)\n",
    "end_date=datetime.datetime(2006,7,1,7,0,0,0)\n",
    "df0 = aa.df_concatenator(dataframes_dict, start_date, end_date, False, True)\n",
    "\n",
    "#df=df.dropna()\n",
    "df=df0.loc[end_date:end_date]\n",
    "\n",
    "lon = df.pivot('y', 'x', 'lon').values\n",
    "lat = df.pivot('y', 'x', 'lat').values\n",
    "u=df.pivot('y', 'x', 'u_scaled_approx').values\n",
    "v=df.pivot('y', 'x', 'v_scaled_approx').values\n",
    "u=np.nan_to_num(u)\n",
    "v=np.nan_to_num(v)\n",
    "qv=df.pivot('y', 'x', 'qv').values*100\n",
    "qv=np.nan_to_num(qv)\n",
    "qv=torch.from_numpy(qv)\n",
    "\n",
    "ut=df.pivot('y', 'x', 'u').values\n",
    "vt=df.pivot('y', 'x', 'v').values\n",
    "ut=np.nan_to_num(ut)\n",
    "vt=np.nan_to_num(vt)\n",
    "\n",
    "speed_error=np.sqrt((u-ut)**2+(v-vt)**2)\n",
    "print('speed_error')\n",
    "print(speed_error.mean())\n",
    "ut=torch.from_numpy(ut)\n",
    "vt=torch.from_numpy(vt)\n",
    "\n",
    "\n",
    "df=df0.loc[start_date:start_date]\n",
    "u0=df.pivot('y', 'x', 'u_scaled_approx').values\n",
    "v0=df.pivot('y', 'x', 'v_scaled_approx').values\n",
    "u0=np.nan_to_num(u0)\n",
    "v0=np.nan_to_num(v0)\n",
    "qv0=df.pivot('y', 'x', 'qv').values*100\n",
    "qv0=np.nan_to_num(qv0)\n",
    "qv0=torch.from_numpy(qv0)\n",
    "\n",
    "u0t=df.pivot('y', 'x', 'u').values\n",
    "v0t=df.pivot('y', 'x', 'v').values\n",
    "u0t=np.nan_to_num(u0t)\n",
    "v0t=np.nan_to_num(v0t)\n",
    "u0t=torch.from_numpy(u0t)\n",
    "v0t=torch.from_numpy(v0t)\n",
    "\n",
    "dx, dy = metpy.calc.lat_lon_grid_deltas(lon, lat)\n",
    "dx=dx.magnitude \n",
    "mask = np.isnan(dx)\n",
    "dx[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), dx[~mask])\n",
    "\n",
    "\n",
    "dy=dy.magnitude \n",
    "mask = np.isnan(dy)\n",
    "dy[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), dy[~mask])\n",
    "\n",
    "\n",
    "mask = dx<1e-15\n",
    "dx[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), dx[~mask])\n",
    "\n",
    "mask = dy<1e-15\n",
    "dy[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), dy[~mask])\n",
    "\n",
    "dx=torch.from_numpy(dx)\n",
    "dy=torch.from_numpy(dy)\n",
    "dx_1=torch.rand(dx.shape)\n",
    "dx_1[:,:]=1\n",
    "dy_1=torch.rand(dy.shape)\n",
    "dy_1[:,:]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 360, 720])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#wind0=torch.rand([1,4,lon.shape[0],lon.shape[1]])\n",
    "wind=torch.rand([1,4,lon.shape[0],lon.shape[1]])\n",
    "wind[0,0,:,:]=torch.from_numpy(u0)\n",
    "wind[0,1,:,:]=torch.from_numpy(v0)\n",
    "wind[0,2,:,:]=torch.from_numpy(u)\n",
    "wind[0,3,:,:]=torch.from_numpy(v)\n",
    "\n",
    "winds=torch.rand([1,2,lon.shape[0],lon.shape[1]])\n",
    "winds[0,0,:,:]=torch.from_numpy(u)\n",
    "winds[0,1,:,:]=torch.from_numpy(v)\n",
    "dt=3600\n",
    "print(wind.shape)\n",
    "dt=3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_central_x(dx, f):\n",
    "    diff=torch.zeros((f.shape[0],f.shape[1]-2))\n",
    "  \n",
    "    for i,_ in enumerate(f): \n",
    "        x_diff = dx[i,1:] + dx[i,:-1]\n",
    "        f_diff = f[i,2:] - f[i,:-2]\n",
    "        diff[i,:]= f_diff/x_diff\n",
    "    difft=torch.zeros(f.shape)\n",
    "    difft[:,1:-1]=diff\n",
    "    return difft\n",
    "\n",
    "\n",
    "def diff_central_y(dy, f):\n",
    "    diff=torch.zeros((f.shape[0]-2,f.shape[1]))\n",
    "  \n",
    "    for i,_ in enumerate(f.T): \n",
    "        y_diff = dy[1:,i] + dy[:-1,i]\n",
    "        f_diff = f[2:,i] - f[:-2,i]\n",
    "        diff[:,i]= f_diff/y_diff\n",
    "    difft=torch.zeros(f.shape)\n",
    "    difft[1:-1,:]=diff\n",
    "    return difft\n",
    "\n",
    "def omega(dx,dy,u,v):\n",
    "    omega=diff_central_x(dx,v)-diff_central_y(dy,u)\n",
    "    return omega\n",
    "\n",
    "def advection(dx,dy,u,v,q):\n",
    "    adv=diff_central_x(dx,(q*u))+diff_central_y(dy,(q*v))\n",
    "    return adv\n",
    "\n",
    "\n",
    "def loss_physics(wind,qv, model,dt):\n",
    "    wind_cnn=wind+model.forward(wind)\n",
    "    u=wind_cnn[:,0,:,:]\n",
    "    v=wind_cnn[:,1,:,:]\n",
    "    \n",
    "    \n",
    "\n",
    "    func=(u-ut)**2+(v-vt)**2\n",
    "    return abs(func.mean())\n",
    "\n",
    "def loss_physics0(wind0,wind,qv, model,dt):\n",
    "    wind_cnn=wind+model.forward(wind)\n",
    "    u=wind_cnn[:,2,:,:]\n",
    "    v=wind_cnn[:,3,:,:]\n",
    "    ug=wind[:,2,:,:]\n",
    "    vg=wind[:,3,:,:]\n",
    "    u=torch.squeeze(u)\n",
    "    v=torch.squeeze(v)\n",
    "    ug=torch.squeeze(ug)\n",
    "    vg=torch.squeeze(vg)\n",
    "    \n",
    "    u0=wind_cnn[:,0,:,:]\n",
    "    v0=wind_cnn[:,1,:,:]\n",
    "    ug0=wind[:,0,:,:]\n",
    "    vg0=wind[:,1,:,:]\n",
    "    u0=torch.squeeze(u0)\n",
    "    v0=torch.squeeze(v0)\n",
    "    ug0=torch.squeeze(ug0)\n",
    "    vg0=torch.squeeze(vg0)\n",
    "    vor0 = omega(dx,dy,u0,v0)\n",
    "    vor = omega(dx,dy,u,v)\n",
    "    \n",
    "    vor_adv = advection(dx,dy,u0,v0,vor0)\n",
    "    \n",
    "    dqdx=diff_central_x(dx_1, qv)\n",
    "    dqdy=diff_central_y(dy_1, qv)\n",
    "    dqdx0=diff_central_x(dx_1, qv0)\n",
    "    dqdy0=diff_central_y(dy_1, qv0)\n",
    "\n",
    "    reg=dqdx*dqdx*(u-ug)**2+dqdy*dqdy*(v-vg)**2\n",
    "    reg0=dqdx0*dqdx0*(u0-ug0)**2+dqdy0*dqdy0*(v0-vg0)**2\n",
    "    barot=vor-vor0-(vor_adv)*dt\n",
    "    \n",
    "    #func=10*(reg)+2e7*barot\n",
    "    #func=5e3*reg\n",
    "    func=1e4*(reg+reg0)+2e7*barot\n",
    "    return abs(func.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs=5, batch_size=1, learning_rate=1e-2):\n",
    "    torch.manual_seed(42)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=learning_rate, \n",
    "                                 weight_decay=1e-5) # <--\n",
    "\n",
    "    outputs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        loss = loss_physics(winds,qv, model,dt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))\n",
    "        #outputs.append((epoch, img, recon),)\n",
    "    #return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Loss:14.6178\n",
      "Epoch:2, Loss:14.3984\n",
      "Epoch:3, Loss:14.2383\n",
      "Epoch:4, Loss:14.0295\n",
      "Epoch:5, Loss:13.8255\n",
      "Epoch:6, Loss:13.7293\n",
      "Epoch:7, Loss:13.6311\n",
      "Epoch:8, Loss:13.5765\n",
      "Epoch:9, Loss:13.5353\n",
      "Epoch:10, Loss:13.4399\n",
      "Epoch:11, Loss:13.3657\n",
      "Epoch:12, Loss:13.2961\n",
      "Epoch:13, Loss:13.2595\n",
      "Epoch:14, Loss:13.2076\n",
      "Epoch:15, Loss:13.1678\n",
      "Epoch:16, Loss:13.1247\n",
      "Epoch:17, Loss:13.0741\n",
      "Epoch:18, Loss:13.0397\n",
      "Epoch:19, Loss:13.0067\n",
      "Epoch:20, Loss:12.9884\n",
      "Epoch:21, Loss:12.9677\n",
      "Epoch:22, Loss:12.9567\n",
      "Epoch:23, Loss:12.9278\n",
      "Epoch:24, Loss:12.9316\n",
      "Epoch:25, Loss:12.9163\n",
      "Epoch:26, Loss:12.8818\n",
      "Epoch:27, Loss:12.9075\n",
      "Epoch:28, Loss:12.8581\n",
      "Epoch:29, Loss:12.8382\n",
      "Epoch:30, Loss:12.8013\n",
      "Epoch:31, Loss:12.8297\n",
      "Epoch:32, Loss:12.8066\n",
      "Epoch:33, Loss:12.8014\n",
      "Epoch:34, Loss:12.7436\n",
      "Epoch:35, Loss:12.7504\n",
      "Epoch:36, Loss:12.7117\n",
      "Epoch:37, Loss:12.6919\n",
      "Epoch:38, Loss:12.6505\n",
      "Epoch:39, Loss:12.6472\n",
      "Epoch:40, Loss:12.6848\n",
      "Epoch:41, Loss:12.6664\n",
      "Epoch:42, Loss:12.6237\n",
      "Epoch:43, Loss:12.6727\n",
      "Epoch:44, Loss:12.5677\n",
      "Epoch:45, Loss:12.5826\n",
      "Epoch:46, Loss:12.5556\n",
      "Epoch:47, Loss:12.5296\n",
      "Epoch:48, Loss:12.5319\n",
      "Epoch:49, Loss:12.5226\n",
      "Epoch:50, Loss:12.5100\n",
      "Epoch:51, Loss:12.5727\n",
      "Epoch:52, Loss:12.5547\n",
      "Epoch:53, Loss:12.4752\n",
      "Epoch:54, Loss:12.4989\n",
      "Epoch:55, Loss:12.4850\n",
      "Epoch:56, Loss:12.4492\n",
      "Epoch:57, Loss:12.4429\n",
      "Epoch:58, Loss:12.4338\n",
      "Epoch:59, Loss:12.4348\n",
      "Epoch:60, Loss:12.4110\n",
      "Epoch:61, Loss:12.3930\n",
      "Epoch:62, Loss:12.3820\n",
      "Epoch:63, Loss:12.3685\n",
      "Epoch:64, Loss:12.3567\n",
      "Epoch:65, Loss:12.3458\n",
      "Epoch:66, Loss:12.3053\n",
      "Epoch:67, Loss:12.2923\n",
      "Epoch:68, Loss:12.3156\n",
      "Epoch:69, Loss:12.3352\n",
      "Epoch:70, Loss:12.2868\n",
      "Epoch:71, Loss:12.2991\n",
      "Epoch:72, Loss:12.2800\n",
      "Epoch:73, Loss:12.2668\n",
      "Epoch:74, Loss:12.2507\n",
      "Epoch:75, Loss:12.2280\n",
      "Epoch:76, Loss:12.2128\n",
      "Epoch:77, Loss:12.2200\n",
      "Epoch:78, Loss:12.2033\n",
      "Epoch:79, Loss:12.1850\n",
      "Epoch:80, Loss:12.1867\n",
      "Epoch:81, Loss:12.1703\n",
      "Epoch:82, Loss:12.1579\n",
      "Epoch:83, Loss:12.1441\n",
      "Epoch:84, Loss:12.1436\n",
      "Epoch:85, Loss:12.1486\n",
      "Epoch:86, Loss:12.1932\n",
      "Epoch:87, Loss:12.1585\n",
      "Epoch:88, Loss:12.1417\n",
      "Epoch:89, Loss:12.1772\n",
      "Epoch:90, Loss:12.1176\n",
      "Epoch:91, Loss:12.1901\n",
      "Epoch:92, Loss:12.2272\n",
      "Epoch:93, Loss:12.1660\n",
      "Epoch:94, Loss:12.2293\n",
      "Epoch:95, Loss:12.1778\n",
      "Epoch:96, Loss:12.1652\n",
      "Epoch:97, Loss:12.1906\n",
      "Epoch:98, Loss:12.1505\n",
      "Epoch:99, Loss:12.1338\n",
      "Epoch:100, Loss:12.1403\n",
      "Epoch:101, Loss:12.1156\n",
      "Epoch:102, Loss:12.1072\n",
      "Epoch:103, Loss:12.0947\n",
      "Epoch:104, Loss:12.0902\n",
      "Epoch:105, Loss:12.0791\n",
      "Epoch:106, Loss:12.0606\n",
      "Epoch:107, Loss:12.0596\n",
      "Epoch:108, Loss:12.0578\n",
      "Epoch:109, Loss:12.0486\n",
      "Epoch:110, Loss:12.0344\n",
      "Epoch:111, Loss:12.0194\n",
      "Epoch:112, Loss:12.0224\n",
      "Epoch:113, Loss:12.0089\n",
      "Epoch:114, Loss:12.0025\n",
      "Epoch:115, Loss:12.0023\n",
      "Epoch:116, Loss:11.9959\n",
      "Epoch:117, Loss:11.9865\n",
      "Epoch:118, Loss:11.9760\n",
      "Epoch:119, Loss:11.9720\n",
      "Epoch:120, Loss:11.9683\n",
      "Epoch:121, Loss:11.9600\n",
      "Epoch:122, Loss:11.9604\n",
      "Epoch:123, Loss:11.9558\n",
      "Epoch:124, Loss:11.9536\n",
      "Epoch:125, Loss:11.9541\n",
      "Epoch:126, Loss:11.9641\n",
      "Epoch:127, Loss:11.9989\n",
      "Epoch:128, Loss:11.9686\n",
      "Epoch:129, Loss:12.0181\n",
      "Epoch:130, Loss:12.0740\n",
      "Epoch:131, Loss:12.0172\n",
      "Epoch:132, Loss:12.0451\n",
      "Epoch:133, Loss:11.9902\n",
      "Epoch:134, Loss:12.0178\n",
      "Epoch:135, Loss:11.9711\n",
      "Epoch:136, Loss:12.0018\n",
      "Epoch:137, Loss:11.9792\n",
      "Epoch:138, Loss:11.9793\n",
      "Epoch:139, Loss:11.9628\n",
      "Epoch:140, Loss:11.9690\n",
      "Epoch:141, Loss:11.9508\n",
      "Epoch:142, Loss:11.9476\n",
      "Epoch:143, Loss:11.9320\n",
      "Epoch:144, Loss:11.9367\n",
      "Epoch:145, Loss:11.9136\n",
      "Epoch:146, Loss:11.9256\n",
      "Epoch:147, Loss:11.9193\n",
      "Epoch:148, Loss:11.9114\n",
      "Epoch:149, Loss:11.9088\n",
      "Epoch:150, Loss:11.8976\n",
      "Epoch:151, Loss:11.9026\n",
      "Epoch:152, Loss:11.8959\n",
      "Epoch:153, Loss:11.8891\n",
      "Epoch:154, Loss:11.8807\n",
      "Epoch:155, Loss:11.8764\n",
      "Epoch:156, Loss:11.8753\n",
      "Epoch:157, Loss:11.8699\n",
      "Epoch:158, Loss:11.8604\n",
      "Epoch:159, Loss:11.8621\n",
      "Epoch:160, Loss:11.8556\n",
      "Epoch:161, Loss:11.8554\n",
      "Epoch:162, Loss:11.8508\n",
      "Epoch:163, Loss:11.8504\n",
      "Epoch:164, Loss:11.8419\n",
      "Epoch:165, Loss:11.8397\n",
      "Epoch:166, Loss:11.8394\n",
      "Epoch:167, Loss:11.8373\n",
      "Epoch:168, Loss:11.8382\n",
      "Epoch:169, Loss:11.8491\n",
      "Epoch:170, Loss:11.8727\n",
      "Epoch:171, Loss:11.8643\n",
      "Epoch:172, Loss:11.8400\n",
      "Epoch:173, Loss:11.8366\n",
      "Epoch:174, Loss:11.8362\n",
      "Epoch:175, Loss:11.8503\n",
      "Epoch:176, Loss:11.8242\n",
      "Epoch:177, Loss:11.8208\n",
      "Epoch:178, Loss:11.8213\n",
      "Epoch:179, Loss:11.8139\n",
      "Epoch:180, Loss:11.8101\n",
      "Epoch:181, Loss:11.8088\n",
      "Epoch:182, Loss:11.8120\n",
      "Epoch:183, Loss:11.8055\n",
      "Epoch:184, Loss:11.8018\n",
      "Epoch:185, Loss:11.8008\n",
      "Epoch:186, Loss:11.7955\n",
      "Epoch:187, Loss:11.7995\n",
      "Epoch:188, Loss:11.8155\n",
      "Epoch:189, Loss:11.8737\n",
      "Epoch:190, Loss:11.8930\n",
      "Epoch:191, Loss:11.8323\n",
      "Epoch:192, Loss:11.8607\n",
      "Epoch:193, Loss:11.8471\n",
      "Epoch:194, Loss:11.8349\n",
      "Epoch:195, Loss:11.8327\n",
      "Epoch:196, Loss:11.8154\n",
      "Epoch:197, Loss:11.8206\n",
      "Epoch:198, Loss:11.8203\n",
      "Epoch:199, Loss:11.8138\n",
      "Epoch:200, Loss:11.7983\n",
      "Epoch:201, Loss:11.8041\n",
      "Epoch:202, Loss:11.8062\n",
      "Epoch:203, Loss:11.7959\n",
      "Epoch:204, Loss:11.7893\n",
      "Epoch:205, Loss:11.7923\n",
      "Epoch:206, Loss:11.7879\n",
      "Epoch:207, Loss:11.7777\n",
      "Epoch:208, Loss:11.7786\n",
      "Epoch:209, Loss:11.7760\n",
      "Epoch:210, Loss:11.7769\n",
      "Epoch:211, Loss:11.7713\n",
      "Epoch:212, Loss:11.7731\n",
      "Epoch:213, Loss:11.7658\n",
      "Epoch:214, Loss:11.7643\n",
      "Epoch:215, Loss:11.7601\n",
      "Epoch:216, Loss:11.7551\n",
      "Epoch:217, Loss:11.7541\n",
      "Epoch:218, Loss:11.7507\n",
      "Epoch:219, Loss:11.7515\n",
      "Epoch:220, Loss:11.7438\n",
      "Epoch:221, Loss:11.7398\n",
      "Epoch:222, Loss:11.7442\n",
      "Epoch:223, Loss:11.7504\n",
      "Epoch:224, Loss:11.7736\n",
      "Epoch:225, Loss:11.8113\n",
      "Epoch:226, Loss:11.7812\n",
      "Epoch:227, Loss:11.7627\n",
      "Epoch:228, Loss:11.7661\n",
      "Epoch:229, Loss:11.8171\n",
      "Epoch:230, Loss:11.8087\n",
      "Epoch:231, Loss:11.7827\n",
      "Epoch:232, Loss:11.7955\n",
      "Epoch:233, Loss:11.7846\n",
      "Epoch:234, Loss:11.7570\n",
      "Epoch:235, Loss:11.7641\n",
      "Epoch:236, Loss:11.7609\n",
      "Epoch:237, Loss:11.7552\n",
      "Epoch:238, Loss:11.7397\n",
      "Epoch:239, Loss:11.7416\n",
      "Epoch:240, Loss:11.7391\n",
      "Epoch:241, Loss:11.7314\n",
      "Epoch:242, Loss:11.7302\n",
      "Epoch:243, Loss:11.7266\n",
      "Epoch:244, Loss:11.7257\n",
      "Epoch:245, Loss:11.7190\n",
      "Epoch:246, Loss:11.7162\n",
      "Epoch:247, Loss:11.7154\n",
      "Epoch:248, Loss:11.7132\n",
      "Epoch:249, Loss:11.7074\n",
      "Epoch:250, Loss:11.7101\n",
      "Epoch:251, Loss:11.7166\n",
      "Epoch:252, Loss:11.7267\n",
      "Epoch:253, Loss:11.7357\n",
      "Epoch:254, Loss:11.7231\n",
      "Epoch:255, Loss:11.7053\n",
      "Epoch:256, Loss:11.7172\n",
      "Epoch:257, Loss:11.7319\n",
      "Epoch:258, Loss:11.7431\n",
      "Epoch:259, Loss:11.7208\n",
      "Epoch:260, Loss:11.7122\n",
      "Epoch:261, Loss:11.7249\n",
      "Epoch:262, Loss:11.7211\n",
      "Epoch:263, Loss:11.7065\n",
      "Epoch:264, Loss:11.7013\n",
      "Epoch:265, Loss:11.7096\n",
      "Epoch:266, Loss:11.7175\n",
      "Epoch:267, Loss:11.7094\n",
      "Epoch:268, Loss:11.7012\n",
      "Epoch:269, Loss:11.7032\n",
      "Epoch:270, Loss:11.7066\n",
      "Epoch:271, Loss:11.6999\n",
      "Epoch:272, Loss:11.6880\n",
      "Epoch:273, Loss:11.6927\n",
      "Epoch:274, Loss:11.6885\n",
      "Epoch:275, Loss:11.6863\n",
      "Epoch:276, Loss:11.6842\n",
      "Epoch:277, Loss:11.6846\n",
      "Epoch:278, Loss:11.6838\n",
      "Epoch:279, Loss:11.6875\n",
      "Epoch:280, Loss:11.6824\n",
      "Epoch:281, Loss:11.6850\n",
      "Epoch:282, Loss:11.6833\n",
      "Epoch:283, Loss:11.6802\n",
      "Epoch:284, Loss:11.6782\n",
      "Epoch:285, Loss:11.6800\n",
      "Epoch:286, Loss:11.6811\n",
      "Epoch:287, Loss:11.6847\n",
      "Epoch:288, Loss:11.6819\n",
      "Epoch:289, Loss:11.6846\n",
      "Epoch:290, Loss:11.6810\n",
      "Epoch:291, Loss:11.6679\n",
      "Epoch:292, Loss:11.6773\n",
      "Epoch:293, Loss:11.6882\n",
      "Epoch:294, Loss:11.7052\n",
      "Epoch:295, Loss:11.6921\n",
      "Epoch:296, Loss:11.6734\n",
      "Epoch:297, Loss:11.6817\n",
      "Epoch:298, Loss:11.6767\n",
      "Epoch:299, Loss:11.6806\n",
      "Epoch:300, Loss:11.6759\n",
      "Epoch:301, Loss:11.6872\n",
      "Epoch:302, Loss:11.7004\n",
      "Epoch:303, Loss:11.7193\n",
      "Epoch:304, Loss:11.7135\n",
      "Epoch:305, Loss:11.7211\n",
      "Epoch:306, Loss:11.7124\n",
      "Epoch:307, Loss:11.6874\n",
      "Epoch:308, Loss:11.7043\n",
      "Epoch:309, Loss:11.7022\n",
      "Epoch:310, Loss:11.6746\n",
      "Epoch:311, Loss:11.6928\n",
      "Epoch:312, Loss:11.6936\n",
      "Epoch:313, Loss:11.6843\n",
      "Epoch:314, Loss:11.6813\n",
      "Epoch:315, Loss:11.6780\n",
      "Epoch:316, Loss:11.6749\n",
      "Epoch:317, Loss:11.6701\n",
      "Epoch:318, Loss:11.6696\n",
      "Epoch:319, Loss:11.6707\n",
      "Epoch:320, Loss:11.6659\n",
      "Epoch:321, Loss:11.6593\n",
      "Epoch:322, Loss:11.6607\n",
      "Epoch:323, Loss:11.6577\n",
      "Epoch:324, Loss:11.6561\n",
      "Epoch:325, Loss:11.6584\n",
      "Epoch:326, Loss:11.6545\n",
      "Epoch:327, Loss:11.6525\n",
      "Epoch:328, Loss:11.6516\n",
      "Epoch:329, Loss:11.6495\n",
      "Epoch:330, Loss:11.6496\n",
      "Epoch:331, Loss:11.6505\n",
      "Epoch:332, Loss:11.6506\n",
      "Epoch:333, Loss:11.6563\n",
      "Epoch:334, Loss:11.6550\n",
      "Epoch:335, Loss:11.6470\n",
      "Epoch:336, Loss:11.6429\n",
      "Epoch:337, Loss:11.6404\n",
      "Epoch:338, Loss:11.6385\n",
      "Epoch:339, Loss:11.6392\n",
      "Epoch:340, Loss:11.6427\n",
      "Epoch:341, Loss:11.6523\n",
      "Epoch:342, Loss:11.6693\n",
      "Epoch:343, Loss:11.7309\n",
      "Epoch:344, Loss:11.7146\n",
      "Epoch:345, Loss:11.6829\n",
      "Epoch:346, Loss:11.7079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:347, Loss:11.7192\n",
      "Epoch:348, Loss:11.6921\n",
      "Epoch:349, Loss:11.7102\n",
      "Epoch:350, Loss:11.6872\n",
      "Epoch:351, Loss:11.6880\n",
      "Epoch:352, Loss:11.6776\n",
      "Epoch:353, Loss:11.6784\n",
      "Epoch:354, Loss:11.6652\n",
      "Epoch:355, Loss:11.6618\n",
      "Epoch:356, Loss:11.6616\n",
      "Epoch:357, Loss:11.6572\n",
      "Epoch:358, Loss:11.6528\n",
      "Epoch:359, Loss:11.6502\n",
      "Epoch:360, Loss:11.6488\n",
      "Epoch:361, Loss:11.6489\n",
      "Epoch:362, Loss:11.6473\n",
      "Epoch:363, Loss:11.6457\n",
      "Epoch:364, Loss:11.6412\n",
      "Epoch:365, Loss:11.6388\n",
      "Epoch:366, Loss:11.6380\n",
      "Epoch:367, Loss:11.6365\n",
      "Epoch:368, Loss:11.6362\n",
      "Epoch:369, Loss:11.6345\n",
      "Epoch:370, Loss:11.6326\n",
      "Epoch:371, Loss:11.6311\n",
      "Epoch:372, Loss:11.6286\n",
      "Epoch:373, Loss:11.6271\n",
      "Epoch:374, Loss:11.6267\n",
      "Epoch:375, Loss:11.6255\n",
      "Epoch:376, Loss:11.6250\n",
      "Epoch:377, Loss:11.6254\n",
      "Epoch:378, Loss:11.6326\n",
      "Epoch:379, Loss:11.6475\n",
      "Epoch:380, Loss:11.6584\n",
      "Epoch:381, Loss:11.6441\n",
      "Epoch:382, Loss:11.6494\n",
      "Epoch:383, Loss:11.6585\n",
      "Epoch:384, Loss:11.6871\n",
      "Epoch:385, Loss:11.6532\n",
      "Epoch:386, Loss:11.6493\n",
      "Epoch:387, Loss:11.6813\n",
      "Epoch:388, Loss:11.6619\n",
      "Epoch:389, Loss:11.6447\n",
      "Epoch:390, Loss:11.6579\n",
      "Epoch:391, Loss:11.6494\n",
      "Epoch:392, Loss:11.6552\n",
      "Epoch:393, Loss:11.6554\n",
      "Epoch:394, Loss:11.6505\n",
      "Epoch:395, Loss:11.6409\n",
      "Epoch:396, Loss:11.6425\n",
      "Epoch:397, Loss:11.6358\n",
      "Epoch:398, Loss:11.6364\n",
      "Epoch:399, Loss:11.6299\n",
      "Epoch:400, Loss:11.6342\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder()\n",
    "#model=Autoenc()\n",
    "max_epochs = 400\n",
    "winds.size()\n",
    "\n",
    "outputs = train(model, num_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2.6899e-02, 4.6902e-03, 4.4163e-03,  ..., 3.3433e-02,\n",
       "           1.2846e-01, 1.2763e-01],\n",
       "          [1.8609e-03, 2.6646e-04, 8.1848e-04,  ..., 3.4463e-02,\n",
       "           7.1362e-02, 1.0795e-01],\n",
       "          [1.9133e-03, 5.3507e-04, 2.9154e-03,  ..., 3.5507e-02,\n",
       "           2.9014e-02, 3.3214e-02],\n",
       "          ...,\n",
       "          [9.9010e-01, 9.9985e-01, 9.9988e-01,  ..., 1.0000e+00,\n",
       "           1.0000e+00, 1.0000e+00],\n",
       "          [8.7225e-01, 9.7525e-01, 9.7479e-01,  ..., 1.0000e+00,\n",
       "           1.0000e+00, 9.9999e-01],\n",
       "          [7.8089e-01, 9.5732e-01, 9.3480e-01,  ..., 1.0000e+00,\n",
       "           9.9996e-01, 9.9988e-01]],\n",
       "\n",
       "         [[1.4080e-01, 7.3020e-02, 5.7864e-02,  ..., 7.6140e-02,\n",
       "           1.9053e-01, 2.0570e-01],\n",
       "          [8.1759e-02, 7.1416e-02, 2.2829e-02,  ..., 5.6646e-03,\n",
       "           6.3555e-03, 8.9281e-02],\n",
       "          [1.1589e-01, 6.0958e-02, 3.8807e-02,  ..., 2.6360e-05,\n",
       "           1.6533e-03, 1.3683e-03],\n",
       "          ...,\n",
       "          [4.1665e-02, 7.0464e-03, 5.3399e-02,  ..., 9.9915e-01,\n",
       "           9.9992e-01, 9.9918e-01],\n",
       "          [9.0512e-02, 4.3643e-02, 6.0081e-02,  ..., 9.9997e-01,\n",
       "           9.9994e-01, 9.9860e-01],\n",
       "          [1.8454e-01, 7.2545e-02, 6.1342e-02,  ..., 9.9727e-01,\n",
       "           9.9730e-01, 9.9304e-01]]]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(winds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed_error\n",
      "tensor(2.5737, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wind_cnn=winds+model.forward(winds)\n",
    "u=wind_cnn[:,0,:,:]\n",
    "v=wind_cnn[:,1,:,:]\n",
    "u=torch.squeeze(u)\n",
    "v=torch.squeeze(v)\n",
    "\n",
    "\n",
    "\n",
    "speed_error=torch.sqrt((u-ut)**2+(v-vt)**2)\n",
    "print('speed_error')\n",
    "print(speed_error.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.]])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "dx=torch.rand([10,9])\n",
    "dy=torch.rand([9,10])\n",
    "\n",
    "dx[:,:]=2\n",
    "dy[:,:]=2\n",
    "\n",
    "f=torch.zeros([10,10])\n",
    "f[:,:5]=1\n",
    "print(f)\n",
    "print(diff_central_x(dx,f))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
