{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will attempt my filter for the first time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    def __init__(self,  scale_factor):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "    def forward(self, x):\n",
    "        return F.interpolate(x, scale_factor=self.scale_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.conv1=nn.Conv2d(2, 16, 3, stride=2, padding=1)\n",
    "        self.conv2=nn.Conv2d(16, 32, 3, stride=2, padding=1)\n",
    "        self.conv3=nn.Conv2d(32, 64, 3)\n",
    "            \n",
    "        self.deconv1=nn.ConvTranspose2d(64, 32, 3)\n",
    "        self.deconv2=nn.ConvTranspose2d(32, 16, 3, stride=(2,2), padding=1, output_padding=1)\n",
    "        self.deconv3=nn.ConvTranspose2d(16, 2, 3, stride=(2,2), padding=1, output_padding=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)),(2,2))\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        \n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        x = torch.sigmoid(self.deconv3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:00<00:00,  6.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating dataframes for all dates for further analysis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed_error\n",
      "3.290381284254803\n"
     ]
    }
   ],
   "source": [
    "from viz import amv_analysis as aa\n",
    "from viz import dataframe_calculators as dfc \n",
    "import datetime\n",
    "import pickle \n",
    "import metpy\n",
    "import numpy as np\n",
    "import torch\n",
    "dict_path = '../data/interim/dictionaries/dataframes.pkl'\n",
    "dataframes_dict = pickle.load(open(dict_path, 'rb'))\n",
    "\n",
    "\n",
    "start_date=datetime.datetime(2006,7,1,6,0,0,0)\n",
    "end_date=datetime.datetime(2006,7,1,7,0,0,0)\n",
    "df0 = aa.df_concatenator(dataframes_dict, start_date, end_date, False, True)\n",
    "\n",
    "#df=df.dropna()\n",
    "df=df0.loc[end_date:end_date]\n",
    "\n",
    "lon = df.pivot('y', 'x', 'lon').values\n",
    "lat = df.pivot('y', 'x', 'lat').values\n",
    "u=df.pivot('y', 'x', 'u_scaled_approx').values\n",
    "v=df.pivot('y', 'x', 'v_scaled_approx').values\n",
    "u=np.nan_to_num(u)\n",
    "v=np.nan_to_num(v)\n",
    "qv=df.pivot('y', 'x', 'qv').values*100\n",
    "qv=np.nan_to_num(qv)\n",
    "qv=torch.from_numpy(qv)\n",
    "\n",
    "ut=df.pivot('y', 'x', 'u').values\n",
    "vt=df.pivot('y', 'x', 'v').values\n",
    "ut=np.nan_to_num(ut)\n",
    "vt=np.nan_to_num(vt)\n",
    "\n",
    "speed_error=np.sqrt((u-ut)**2+(v-vt)**2)\n",
    "print('speed_error')\n",
    "print(speed_error.mean())\n",
    "ut=torch.from_numpy(ut)\n",
    "vt=torch.from_numpy(vt)\n",
    "\n",
    "\n",
    "df=df0.loc[start_date:start_date]\n",
    "u0=df.pivot('y', 'x', 'u_scaled_approx').values\n",
    "v0=df.pivot('y', 'x', 'v_scaled_approx').values\n",
    "u0=np.nan_to_num(u0)\n",
    "v0=np.nan_to_num(v0)\n",
    "qv0=df.pivot('y', 'x', 'qv').values*100\n",
    "qv0=np.nan_to_num(qv0)\n",
    "qv0=torch.from_numpy(qv0)\n",
    "\n",
    "u0t=df.pivot('y', 'x', 'u').values\n",
    "v0t=df.pivot('y', 'x', 'v').values\n",
    "u0t=np.nan_to_num(u0t)\n",
    "v0t=np.nan_to_num(v0t)\n",
    "u0t=torch.from_numpy(u0t)\n",
    "v0t=torch.from_numpy(v0t)\n",
    "\n",
    "dx, dy = metpy.calc.lat_lon_grid_deltas(lon, lat)\n",
    "dx=dx.magnitude \n",
    "mask = np.isnan(dx)\n",
    "dx[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), dx[~mask])\n",
    "\n",
    "\n",
    "dy=dy.magnitude \n",
    "mask = np.isnan(dy)\n",
    "dy[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), dy[~mask])\n",
    "\n",
    "\n",
    "mask = dx<1e-15\n",
    "dx[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), dx[~mask])\n",
    "\n",
    "mask = dy<1e-15\n",
    "dy[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), dy[~mask])\n",
    "\n",
    "dx=torch.from_numpy(dx)\n",
    "dy=torch.from_numpy(dy)\n",
    "dx_1=torch.rand(dx.shape)\n",
    "dx_1[:,:]=1\n",
    "dy_1=torch.rand(dy.shape)\n",
    "dy_1[:,:]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#wind0=torch.rand([1,4,lon.shape[0],lon.shape[1]])\n",
    "\n",
    "#wind=torch.rand([1,4,lon.shape[0],lon.shape[1]])\n",
    "#wind[0,0,:,:]=torch.from_numpy(u0)\n",
    "#wind[0,1,:,:]=torch.from_numpy(v0)\n",
    "#wind[0,2,:,:]=torch.from_numpy(u)\n",
    "#wind[0,3,:,:]=torch.from_numpy(v)\n",
    "\n",
    "winds=torch.rand([1,2,lon.shape[0],lon.shape[1]])\n",
    "winds[0,0,:,:]=torch.from_numpy(u)\n",
    "winds[0,1,:,:]=torch.from_numpy(v)\n",
    "dt=3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_central_x(dx, f):\n",
    "    diff=torch.zeros((f.shape[0],f.shape[1]-2))\n",
    "  \n",
    "    for i,_ in enumerate(f): \n",
    "        x_diff = dx[i,1:] + dx[i,:-1]\n",
    "        f_diff = f[i,2:] - f[i,:-2]\n",
    "        diff[i,:]= f_diff/x_diff\n",
    "    difft=torch.zeros(f.shape)\n",
    "    difft[:,1:-1]=diff\n",
    "    return difft\n",
    "\n",
    "\n",
    "def diff_central_y(dy, f):\n",
    "    diff=torch.zeros((f.shape[0]-2,f.shape[1]))\n",
    "  \n",
    "    for i,_ in enumerate(f.T): \n",
    "        y_diff = dy[1:,i] + dy[:-1,i]\n",
    "        f_diff = f[2:,i] - f[:-2,i]\n",
    "        diff[:,i]= f_diff/y_diff\n",
    "    difft=torch.zeros(f.shape)\n",
    "    difft[1:-1,:]=diff\n",
    "    return difft\n",
    "\n",
    "def div(u,v,dx,dy):\n",
    "    div= diff_central_y(dy, v)+diff_central_x(dx, u)\n",
    "    return(div)\n",
    "\n",
    "def omega(dx,dy,u,v):\n",
    "    omega=diff_central_x(dx,v)-diff_central_y(dy,u)\n",
    "    return omega\n",
    "\n",
    "def advection(dx,dy,u,v,q):\n",
    "    adv=diff_central_x(dx,(q*u))+diff_central_y(dy,(q*v))\n",
    "    return adv\n",
    "\n",
    "\n",
    "def loss_physics(wind,qv, model,dt):\n",
    "    wind_cnn=winds+model.forward(winds)\n",
    "    u=wind_cnn[0,0,:,:]\n",
    "    v=wind_cnn[0,1,:,:]\n",
    "    divf=1e6*abs(div(u,v,dx,dy))\n",
    "    #print(divf.mean())\n",
    "    func=(u-ut)**2+(v-vt)**2 #+ divf\n",
    "    return abs(func.mean())\n",
    "\n",
    "def loss_physics0(wind0,wind,qv, model,dt):\n",
    "    wind_cnn=wind+model.forward(wind)\n",
    "    u=wind_cnn[:,2,:,:]\n",
    "    v=wind_cnn[:,3,:,:]\n",
    "    ug=wind[:,2,:,:]\n",
    "    vg=wind[:,3,:,:]\n",
    "    u=torch.squeeze(u)\n",
    "    v=torch.squeeze(v)\n",
    "    ug=torch.squeeze(ug)\n",
    "    vg=torch.squeeze(vg)\n",
    "    \n",
    "    u0=wind_cnn[:,0,:,:]\n",
    "    v0=wind_cnn[:,1,:,:]\n",
    "    ug0=wind[:,0,:,:]\n",
    "    vg0=wind[:,1,:,:]\n",
    "    u0=torch.squeeze(u0)\n",
    "    v0=torch.squeeze(v0)\n",
    "    ug0=torch.squeeze(ug0)\n",
    "    vg0=torch.squeeze(vg0)\n",
    "    vor0 = omega(dx,dy,u0,v0)\n",
    "    vor = omega(dx,dy,u,v)\n",
    "    \n",
    "    vor_adv = advection(dx,dy,u0,v0,vor0)\n",
    "    \n",
    "    dqdx=diff_central_x(dx_1, qv)\n",
    "    dqdy=diff_central_y(dy_1, qv)\n",
    "    dqdx0=diff_central_x(dx_1, qv0)\n",
    "    dqdy0=diff_central_y(dy_1, qv0)\n",
    "\n",
    "    reg=dqdx*dqdx*(u-ug)**2+dqdy*dqdy*(v-vg)**2\n",
    "    reg0=dqdx0*dqdx0*(u0-ug0)**2+dqdy0*dqdy0*(v0-vg0)**2\n",
    "    barot=vor-vor0-(vor_adv)*dt\n",
    "    \n",
    "    #func=10*(reg)+2e7*barot\n",
    "    #func=5e3*reg\n",
    "    func=1e4*(reg+reg0)+2e7*barot\n",
    "    return abs(func.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs=5, batch_size=1, learning_rate=1e-2):\n",
    "    torch.manual_seed(42)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=learning_rate, \n",
    "                                 weight_decay=1e-5) # <--\n",
    "\n",
    "    outputs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        loss = loss_physics(winds,qv, model,dt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))\n",
    "        #outputs.append((epoch, img, recon),)\n",
    "    #return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Loss:24.0175\n",
      "Epoch:2, Loss:23.9522\n",
      "Epoch:3, Loss:23.6953\n",
      "Epoch:4, Loss:23.4839\n",
      "Epoch:5, Loss:23.3716\n",
      "Epoch:6, Loss:23.3955\n",
      "Epoch:7, Loss:23.3130\n",
      "Epoch:8, Loss:23.2193\n",
      "Epoch:9, Loss:23.2495\n",
      "Epoch:10, Loss:23.2038\n",
      "Epoch:11, Loss:23.1400\n",
      "Epoch:12, Loss:23.2122\n",
      "Epoch:13, Loss:23.0391\n",
      "Epoch:14, Loss:23.0190\n",
      "Epoch:15, Loss:22.8957\n",
      "Epoch:16, Loss:22.8406\n",
      "Epoch:17, Loss:22.7472\n",
      "Epoch:18, Loss:22.6990\n",
      "Epoch:19, Loss:22.6653\n",
      "Epoch:20, Loss:22.6172\n",
      "Epoch:21, Loss:22.5967\n",
      "Epoch:22, Loss:22.5787\n",
      "Epoch:23, Loss:22.5921\n",
      "Epoch:24, Loss:22.5596\n",
      "Epoch:25, Loss:22.5485\n",
      "Epoch:26, Loss:22.5347\n",
      "Epoch:27, Loss:22.5165\n",
      "Epoch:28, Loss:22.5098\n",
      "Epoch:29, Loss:22.4996\n",
      "Epoch:30, Loss:22.4762\n",
      "Epoch:31, Loss:22.4537\n",
      "Epoch:32, Loss:22.4321\n",
      "Epoch:33, Loss:22.4220\n",
      "Epoch:34, Loss:22.4039\n",
      "Epoch:35, Loss:22.3939\n",
      "Epoch:36, Loss:22.3824\n",
      "Epoch:37, Loss:22.3740\n",
      "Epoch:38, Loss:22.3722\n",
      "Epoch:39, Loss:22.3438\n",
      "Epoch:40, Loss:22.3625\n",
      "Epoch:41, Loss:22.3339\n",
      "Epoch:42, Loss:22.3240\n",
      "Epoch:43, Loss:22.2999\n",
      "Epoch:44, Loss:22.3117\n",
      "Epoch:45, Loss:22.3175\n",
      "Epoch:46, Loss:22.2975\n",
      "Epoch:47, Loss:22.3009\n",
      "Epoch:48, Loss:22.2896\n",
      "Epoch:49, Loss:22.2851\n",
      "Epoch:50, Loss:22.2832\n",
      "Epoch:51, Loss:22.3273\n",
      "Epoch:52, Loss:22.3137\n",
      "Epoch:53, Loss:22.2917\n",
      "Epoch:54, Loss:22.2755\n",
      "Epoch:55, Loss:22.2640\n",
      "Epoch:56, Loss:22.2593\n",
      "Epoch:57, Loss:22.2513\n",
      "Epoch:58, Loss:22.2293\n",
      "Epoch:59, Loss:22.2172\n",
      "Epoch:60, Loss:22.2031\n",
      "Epoch:61, Loss:22.1963\n",
      "Epoch:62, Loss:22.1991\n",
      "Epoch:63, Loss:22.1887\n",
      "Epoch:64, Loss:22.1570\n",
      "Epoch:65, Loss:22.1651\n",
      "Epoch:66, Loss:22.1318\n",
      "Epoch:67, Loss:22.1327\n",
      "Epoch:68, Loss:22.0939\n",
      "Epoch:69, Loss:22.1202\n",
      "Epoch:70, Loss:22.0908\n",
      "Epoch:71, Loss:22.0672\n",
      "Epoch:72, Loss:22.0573\n",
      "Epoch:73, Loss:22.0408\n",
      "Epoch:74, Loss:22.0804\n",
      "Epoch:75, Loss:22.0560\n",
      "Epoch:76, Loss:22.0460\n",
      "Epoch:77, Loss:22.0298\n",
      "Epoch:78, Loss:22.0172\n",
      "Epoch:79, Loss:22.0058\n",
      "Epoch:80, Loss:21.9893\n",
      "Epoch:81, Loss:22.0403\n",
      "Epoch:82, Loss:22.0239\n",
      "Epoch:83, Loss:22.0356\n",
      "Epoch:84, Loss:22.0075\n",
      "Epoch:85, Loss:21.9851\n",
      "Epoch:86, Loss:21.9956\n",
      "Epoch:87, Loss:21.9567\n",
      "Epoch:88, Loss:21.9632\n",
      "Epoch:89, Loss:21.9520\n",
      "Epoch:90, Loss:21.9336\n",
      "Epoch:91, Loss:21.9261\n",
      "Epoch:92, Loss:21.9419\n",
      "Epoch:93, Loss:21.9366\n",
      "Epoch:94, Loss:21.9218\n",
      "Epoch:95, Loss:21.9444\n",
      "Epoch:96, Loss:21.9006\n",
      "Epoch:97, Loss:21.9117\n",
      "Epoch:98, Loss:21.9335\n",
      "Epoch:99, Loss:21.9195\n",
      "Epoch:100, Loss:21.8835\n",
      "Epoch:101, Loss:21.8918\n",
      "Epoch:102, Loss:21.8642\n",
      "Epoch:103, Loss:21.8598\n",
      "Epoch:104, Loss:21.8621\n",
      "Epoch:105, Loss:21.8251\n",
      "Epoch:106, Loss:21.8304\n",
      "Epoch:107, Loss:21.8454\n",
      "Epoch:108, Loss:21.8751\n",
      "Epoch:109, Loss:21.8557\n",
      "Epoch:110, Loss:21.8422\n",
      "Epoch:111, Loss:21.8789\n",
      "Epoch:112, Loss:21.8582\n",
      "Epoch:113, Loss:21.8662\n",
      "Epoch:114, Loss:21.8691\n",
      "Epoch:115, Loss:21.8553\n",
      "Epoch:116, Loss:21.8694\n",
      "Epoch:117, Loss:21.8407\n",
      "Epoch:118, Loss:21.8276\n",
      "Epoch:119, Loss:21.8361\n",
      "Epoch:120, Loss:21.8549\n",
      "Epoch:121, Loss:21.8488\n",
      "Epoch:122, Loss:21.8139\n",
      "Epoch:123, Loss:21.8233\n",
      "Epoch:124, Loss:21.8477\n",
      "Epoch:125, Loss:21.8706\n",
      "Epoch:126, Loss:21.8719\n",
      "Epoch:127, Loss:21.8284\n",
      "Epoch:128, Loss:21.8326\n",
      "Epoch:129, Loss:21.8233\n",
      "Epoch:130, Loss:21.7913\n",
      "Epoch:131, Loss:21.7446\n",
      "Epoch:132, Loss:21.7588\n",
      "Epoch:133, Loss:21.7864\n",
      "Epoch:134, Loss:21.7620\n",
      "Epoch:135, Loss:21.7690\n",
      "Epoch:136, Loss:21.7519\n",
      "Epoch:137, Loss:21.7497\n",
      "Epoch:138, Loss:21.7451\n",
      "Epoch:139, Loss:21.7307\n",
      "Epoch:140, Loss:21.7220\n",
      "Epoch:141, Loss:21.7048\n",
      "Epoch:142, Loss:21.6926\n",
      "Epoch:143, Loss:21.6787\n",
      "Epoch:144, Loss:21.6688\n",
      "Epoch:145, Loss:21.6564\n",
      "Epoch:146, Loss:21.6429\n",
      "Epoch:147, Loss:21.6370\n",
      "Epoch:148, Loss:21.6307\n",
      "Epoch:149, Loss:21.6148\n",
      "Epoch:150, Loss:21.6115\n",
      "Epoch:151, Loss:21.6105\n",
      "Epoch:152, Loss:21.6503\n",
      "Epoch:153, Loss:21.6017\n",
      "Epoch:154, Loss:21.7150\n",
      "Epoch:155, Loss:21.6806\n",
      "Epoch:156, Loss:21.7297\n",
      "Epoch:157, Loss:21.6871\n",
      "Epoch:158, Loss:21.6859\n",
      "Epoch:159, Loss:21.6867\n",
      "Epoch:160, Loss:21.6517\n",
      "Epoch:161, Loss:21.6448\n",
      "Epoch:162, Loss:21.6207\n",
      "Epoch:163, Loss:21.5921\n",
      "Epoch:164, Loss:21.5868\n",
      "Epoch:165, Loss:21.5870\n",
      "Epoch:166, Loss:21.5926\n",
      "Epoch:167, Loss:21.6059\n",
      "Epoch:168, Loss:21.6012\n",
      "Epoch:169, Loss:21.5934\n",
      "Epoch:170, Loss:21.5778\n",
      "Epoch:171, Loss:21.5791\n",
      "Epoch:172, Loss:21.5571\n",
      "Epoch:173, Loss:21.5424\n",
      "Epoch:174, Loss:21.5384\n",
      "Epoch:175, Loss:21.5213\n",
      "Epoch:176, Loss:21.5410\n",
      "Epoch:177, Loss:21.5643\n",
      "Epoch:178, Loss:21.5863\n",
      "Epoch:179, Loss:21.5765\n",
      "Epoch:180, Loss:21.5882\n",
      "Epoch:181, Loss:21.5673\n",
      "Epoch:182, Loss:21.5579\n",
      "Epoch:183, Loss:21.5393\n",
      "Epoch:184, Loss:21.5337\n",
      "Epoch:185, Loss:21.5232\n",
      "Epoch:186, Loss:21.5105\n",
      "Epoch:187, Loss:21.5046\n",
      "Epoch:188, Loss:21.5070\n",
      "Epoch:189, Loss:21.4925\n",
      "Epoch:190, Loss:21.4885\n",
      "Epoch:191, Loss:21.4983\n",
      "Epoch:192, Loss:21.4886\n",
      "Epoch:193, Loss:21.4665\n",
      "Epoch:194, Loss:21.4889\n",
      "Epoch:195, Loss:21.4971\n",
      "Epoch:196, Loss:21.4682\n",
      "Epoch:197, Loss:21.4715\n",
      "Epoch:198, Loss:21.4519\n",
      "Epoch:199, Loss:21.4490\n",
      "Epoch:200, Loss:21.4520\n",
      "Epoch:201, Loss:21.4408\n",
      "Epoch:202, Loss:21.4253\n",
      "Epoch:203, Loss:21.4357\n",
      "Epoch:204, Loss:21.4441\n",
      "Epoch:205, Loss:21.4453\n",
      "Epoch:206, Loss:21.4593\n",
      "Epoch:207, Loss:21.4248\n",
      "Epoch:208, Loss:21.4391\n",
      "Epoch:209, Loss:21.4177\n",
      "Epoch:210, Loss:21.4145\n",
      "Epoch:211, Loss:21.4121\n",
      "Epoch:212, Loss:21.3990\n",
      "Epoch:213, Loss:21.3934\n",
      "Epoch:214, Loss:21.3886\n",
      "Epoch:215, Loss:21.4138\n",
      "Epoch:216, Loss:21.3945\n",
      "Epoch:217, Loss:21.3844\n",
      "Epoch:218, Loss:21.3763\n",
      "Epoch:219, Loss:21.3648\n",
      "Epoch:220, Loss:21.3571\n",
      "Epoch:221, Loss:21.3590\n",
      "Epoch:222, Loss:21.3544\n",
      "Epoch:223, Loss:21.3625\n",
      "Epoch:224, Loss:21.3551\n",
      "Epoch:225, Loss:21.3433\n",
      "Epoch:226, Loss:21.3389\n",
      "Epoch:227, Loss:21.3234\n",
      "Epoch:228, Loss:21.3468\n",
      "Epoch:229, Loss:21.3701\n",
      "Epoch:230, Loss:21.3900\n",
      "Epoch:231, Loss:21.3838\n",
      "Epoch:232, Loss:21.3381\n",
      "Epoch:233, Loss:21.3512\n",
      "Epoch:234, Loss:21.3937\n",
      "Epoch:235, Loss:21.3423\n",
      "Epoch:236, Loss:21.3822\n",
      "Epoch:237, Loss:21.3340\n",
      "Epoch:238, Loss:21.3412\n",
      "Epoch:239, Loss:21.3322\n",
      "Epoch:240, Loss:21.3254\n",
      "Epoch:241, Loss:21.3302\n",
      "Epoch:242, Loss:21.3170\n",
      "Epoch:243, Loss:21.3198\n",
      "Epoch:244, Loss:21.2980\n",
      "Epoch:245, Loss:21.2986\n",
      "Epoch:246, Loss:21.2966\n",
      "Epoch:247, Loss:21.2815\n",
      "Epoch:248, Loss:21.2832\n",
      "Epoch:249, Loss:21.2813\n",
      "Epoch:250, Loss:21.2734\n",
      "Epoch:251, Loss:21.2752\n",
      "Epoch:252, Loss:21.2604\n",
      "Epoch:253, Loss:21.2638\n",
      "Epoch:254, Loss:21.2669\n",
      "Epoch:255, Loss:21.2683\n",
      "Epoch:256, Loss:21.2737\n",
      "Epoch:257, Loss:21.2595\n",
      "Epoch:258, Loss:21.2758\n",
      "Epoch:259, Loss:21.3188\n",
      "Epoch:260, Loss:21.2841\n",
      "Epoch:261, Loss:21.2751\n",
      "Epoch:262, Loss:21.2882\n",
      "Epoch:263, Loss:21.2859\n",
      "Epoch:264, Loss:21.2957\n",
      "Epoch:265, Loss:21.2980\n",
      "Epoch:266, Loss:21.2896\n",
      "Epoch:267, Loss:21.3081\n",
      "Epoch:268, Loss:21.2783\n",
      "Epoch:269, Loss:21.3041\n",
      "Epoch:270, Loss:21.2489\n",
      "Epoch:271, Loss:21.2911\n",
      "Epoch:272, Loss:21.2635\n",
      "Epoch:273, Loss:21.2700\n",
      "Epoch:274, Loss:21.2471\n",
      "Epoch:275, Loss:21.2714\n",
      "Epoch:276, Loss:21.2498\n",
      "Epoch:277, Loss:21.2425\n",
      "Epoch:278, Loss:21.2408\n",
      "Epoch:279, Loss:21.2302\n",
      "Epoch:280, Loss:21.2161\n",
      "Epoch:281, Loss:21.2149\n",
      "Epoch:282, Loss:21.2043\n",
      "Epoch:283, Loss:21.2091\n",
      "Epoch:284, Loss:21.1951\n",
      "Epoch:285, Loss:21.1932\n",
      "Epoch:286, Loss:21.1951\n",
      "Epoch:287, Loss:21.1844\n",
      "Epoch:288, Loss:21.1833\n",
      "Epoch:289, Loss:21.1750\n",
      "Epoch:290, Loss:21.1727\n",
      "Epoch:291, Loss:21.1752\n",
      "Epoch:292, Loss:21.1724\n",
      "Epoch:293, Loss:21.1678\n",
      "Epoch:294, Loss:21.1690\n",
      "Epoch:295, Loss:21.1627\n",
      "Epoch:296, Loss:21.1642\n",
      "Epoch:297, Loss:21.1632\n",
      "Epoch:298, Loss:21.1682\n",
      "Epoch:299, Loss:21.1909\n",
      "Epoch:300, Loss:21.2210\n",
      "Epoch:301, Loss:21.2948\n",
      "Epoch:302, Loss:21.3049\n",
      "Epoch:303, Loss:21.2316\n",
      "Epoch:304, Loss:21.2488\n",
      "Epoch:305, Loss:21.2232\n",
      "Epoch:306, Loss:21.2262\n",
      "Epoch:307, Loss:21.2296\n",
      "Epoch:308, Loss:21.2049\n",
      "Epoch:309, Loss:21.2622\n",
      "Epoch:310, Loss:21.2980\n",
      "Epoch:311, Loss:21.2480\n",
      "Epoch:312, Loss:21.2783\n",
      "Epoch:313, Loss:21.2633\n",
      "Epoch:314, Loss:21.2318\n",
      "Epoch:315, Loss:21.2333\n",
      "Epoch:316, Loss:21.2130\n",
      "Epoch:317, Loss:21.2205\n",
      "Epoch:318, Loss:21.2193\n",
      "Epoch:319, Loss:21.2058\n",
      "Epoch:320, Loss:21.1862\n",
      "Epoch:321, Loss:21.1986\n",
      "Epoch:322, Loss:21.2111\n",
      "Epoch:323, Loss:21.2002\n",
      "Epoch:324, Loss:21.1899\n",
      "Epoch:325, Loss:21.1941\n",
      "Epoch:326, Loss:21.1606\n",
      "Epoch:327, Loss:21.1666\n",
      "Epoch:328, Loss:21.1424\n",
      "Epoch:329, Loss:21.1566\n",
      "Epoch:330, Loss:21.1436\n",
      "Epoch:331, Loss:21.1405\n",
      "Epoch:332, Loss:21.1370\n",
      "Epoch:333, Loss:21.1332\n",
      "Epoch:334, Loss:21.1268\n",
      "Epoch:335, Loss:21.1131\n",
      "Epoch:336, Loss:21.1229\n",
      "Epoch:337, Loss:21.1101\n",
      "Epoch:338, Loss:21.1158\n",
      "Epoch:339, Loss:21.1107\n",
      "Epoch:340, Loss:21.0996\n",
      "Epoch:341, Loss:21.1093\n",
      "Epoch:342, Loss:21.1247\n",
      "Epoch:343, Loss:21.1158\n",
      "Epoch:344, Loss:21.1107\n",
      "Epoch:345, Loss:21.1164\n",
      "Epoch:346, Loss:21.1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:347, Loss:21.0978\n",
      "Epoch:348, Loss:21.1165\n",
      "Epoch:349, Loss:21.1092\n",
      "Epoch:350, Loss:21.0962\n",
      "Epoch:351, Loss:21.0904\n",
      "Epoch:352, Loss:21.0883\n",
      "Epoch:353, Loss:21.0952\n",
      "Epoch:354, Loss:21.0853\n",
      "Epoch:355, Loss:21.0974\n",
      "Epoch:356, Loss:21.1155\n",
      "Epoch:357, Loss:21.0868\n",
      "Epoch:358, Loss:21.0889\n",
      "Epoch:359, Loss:21.1225\n",
      "Epoch:360, Loss:21.0934\n",
      "Epoch:361, Loss:21.0973\n",
      "Epoch:362, Loss:21.0896\n",
      "Epoch:363, Loss:21.0753\n",
      "Epoch:364, Loss:21.0790\n",
      "Epoch:365, Loss:21.0623\n",
      "Epoch:366, Loss:21.0749\n",
      "Epoch:367, Loss:21.0674\n",
      "Epoch:368, Loss:21.0753\n",
      "Epoch:369, Loss:21.0616\n",
      "Epoch:370, Loss:21.0545\n",
      "Epoch:371, Loss:21.0559\n",
      "Epoch:372, Loss:21.0558\n",
      "Epoch:373, Loss:21.0562\n",
      "Epoch:374, Loss:21.0540\n",
      "Epoch:375, Loss:21.0626\n",
      "Epoch:376, Loss:21.0550\n",
      "Epoch:377, Loss:21.0552\n",
      "Epoch:378, Loss:21.0598\n",
      "Epoch:379, Loss:21.0627\n",
      "Epoch:380, Loss:21.0560\n",
      "Epoch:381, Loss:21.0445\n",
      "Epoch:382, Loss:21.0493\n",
      "Epoch:383, Loss:21.0399\n",
      "Epoch:384, Loss:21.0491\n",
      "Epoch:385, Loss:21.0656\n",
      "Epoch:386, Loss:21.1180\n",
      "Epoch:387, Loss:21.1725\n",
      "Epoch:388, Loss:21.2143\n",
      "Epoch:389, Loss:21.1543\n",
      "Epoch:390, Loss:21.1229\n",
      "Epoch:391, Loss:21.1411\n",
      "Epoch:392, Loss:21.1210\n",
      "Epoch:393, Loss:21.0934\n",
      "Epoch:394, Loss:21.1122\n",
      "Epoch:395, Loss:21.0879\n",
      "Epoch:396, Loss:21.0803\n",
      "Epoch:397, Loss:21.0779\n",
      "Epoch:398, Loss:21.0883\n",
      "Epoch:399, Loss:21.0640\n",
      "Epoch:400, Loss:21.0554\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder()\n",
    "\n",
    "\n",
    "#model=Autoenc()\n",
    "max_epochs = 400\n",
    "winds.size()\n",
    "\n",
    "outputs = train(model, num_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00, -7.5070e-23, -1.2985e-22,  ...,  2.8817e-22,\n",
       "          1.5658e-22,  0.0000e+00],\n",
       "        [-1.6223e-08, -2.1537e-08, -2.5342e-08,  ..., -5.3028e-09,\n",
       "         -1.4483e-08, -2.5893e-08],\n",
       "        [-3.0082e-08, -3.5317e-08, -3.9078e-08,  ..., -2.7877e-08,\n",
       "         -3.7574e-08, -4.7763e-08],\n",
       "        ...,\n",
       "        [-7.1254e-08, -7.4898e-08, -1.5430e-07,  ..., -3.9637e-08,\n",
       "         -5.9475e-08, -8.0500e-08],\n",
       "        [-4.1528e-08, -4.8149e-09, -5.8392e-08,  ..., -4.8251e-08,\n",
       "         -5.4220e-08, -7.2181e-08],\n",
       "        [ 0.0000e+00,  5.3563e-08,  3.9497e-08,  ...,  5.3932e-08,\n",
       "          3.3154e-08,  0.0000e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div(winds[0,0,:,:],winds[0,1,:,:],dx,dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0530e-06, 1.0331e-09, 1.7640e-10,  ..., 4.2897e-08,\n",
       "           1.6901e-05, 5.5808e-04],\n",
       "          [1.4266e-10, 3.9944e-17, 7.7476e-14,  ..., 1.3187e-13,\n",
       "           2.2953e-07, 9.0532e-08],\n",
       "          [2.8433e-10, 8.8393e-15, 4.5846e-15,  ..., 1.6379e-11,\n",
       "           7.9097e-09, 1.2008e-05],\n",
       "          ...,\n",
       "          [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
       "           1.0000e+00, 1.0000e+00],\n",
       "          [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
       "           1.0000e+00, 9.9966e-01],\n",
       "          [9.9943e-01, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
       "           9.9994e-01, 9.9471e-01]],\n",
       "\n",
       "         [[5.8016e-03, 1.0545e-05, 3.6657e-05,  ..., 3.1679e-04,\n",
       "           9.1882e-03, 2.3536e-02],\n",
       "          [8.7066e-04, 6.7682e-08, 6.4045e-07,  ..., 7.6352e-11,\n",
       "           9.3734e-08, 1.2664e-05],\n",
       "          [6.1236e-03, 2.3612e-05, 4.8646e-06,  ..., 1.1128e-11,\n",
       "           2.7927e-09, 1.1222e-06],\n",
       "          ...,\n",
       "          [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
       "           1.0000e+00, 1.0000e+00],\n",
       "          [9.9996e-01, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
       "           1.0000e+00, 9.9976e-01],\n",
       "          [9.9927e-01, 1.0000e+00, 9.9999e-01,  ..., 1.0000e+00,\n",
       "           9.9991e-01, 9.9959e-01]]]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(winds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed_error\n",
      "tensor(3.0716, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wind_cnn=winds+model.forward(winds)\n",
    "u=wind_cnn[:,0,:,:]\n",
    "v=wind_cnn[:,1,:,:]\n",
    "u=torch.squeeze(u)\n",
    "v=torch.squeeze(v)\n",
    "\n",
    "\n",
    "\n",
    "speed_error=torch.sqrt((u-ut)**2+(v-vt)**2)\n",
    "print('speed_error')\n",
    "print(speed_error.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.]])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500, -0.2500,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "dx=torch.rand([10,9])\n",
    "dy=torch.rand([9,10])\n",
    "\n",
    "dx[:,:]=2\n",
    "dy[:,:]=2\n",
    "\n",
    "f=torch.zeros([10,10])\n",
    "f[:,:5]=1\n",
    "print(f)\n",
    "print(diff_central_x(dx,f))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with div*1e6\n",
    "speed_error\n",
    "tensor(4.2333, grad_fn=<MeanBackward0>)\n",
    "without div\n",
    "speed_error\n",
    "tensor(4.0462, grad_fn=<MeanBackward0>)\n",
    "speed_error\n",
    "tensor(3.0716, grad_fn=<MeanBackward0>)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
